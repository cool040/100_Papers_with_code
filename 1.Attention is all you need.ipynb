{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c17e664",
   "metadata": {},
   "source": [
    "### 1.Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = torch.exp(z)\n",
    "    return exp_z / torch.sum(exp_z)    #！感觉不对\n",
    "\n",
    "def dot_product_matrix(A, B):\n",
    "    return torch.matmul(A, B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087c403",
   "metadata": {},
   "source": [
    "### 2. transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n",
    "        ...\n",
    "    \n",
    "    def forward(self, original, target):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88b61b",
   "metadata": {},
   "source": [
    "#### 2.1 Word Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68335488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class navie_embedding(nn.Module):\n",
    "    def __init__(self, v, d):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Parameter(torch.randn(v, d)) # 初始化Embedding Table\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        # 第一种方法: \n",
    "        return self.embedding[x]  # 直接索引获取嵌入向量\n",
    "\n",
    "        # 第二种方法: One Hot Encoding\n",
    "        # x_one_hot = F.one_hot(x, num_classes=self.embedding.size(0)).float() # (batch_size, seq_len, v)\n",
    "        # return torch.matmul(x_one_hot, self.embedding) # (batch_size, seq_len, d)\n",
    "\n",
    "        # 第三种方法，利用Gather函数\n",
    "        # batch_size, seq_len = x.size()\n",
    "        # x = x.unsqueeze(-1).expand(-1, -1, self.embedding.size(1)) # (batch_size, seq_len, d)\n",
    "        # return torch.gather(self.embedding.unsqueeze(0).expand(batch_size, -1, -1), 1, x) # (batch_size, seq_len, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085782f",
   "metadata": {},
   "source": [
    "#### 2.2 Position Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8990b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 1\n",
    "d_model = 6\n",
    "pe = torch.zeros(d_model)\n",
    "for i in range(d_model // 2):\n",
    "    pe[2 * i] = torch.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "    pe[2 * i + 1] = torch.cos(pos / (10000 ** (2 * i + 1 / d_model)))\n",
    "print(pe)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) # (d_model/2,)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203add91",
   "metadata": {},
   "source": [
    "#### 2.3 Attention Layer\n",
    "\n",
    "Attention机制是Transformer的核心组件，它允许模型在处理序列时动态地关注输入序列中的不同部分。Attention机制的基本思想是通过计算查询（Query）、键（Key）和值（Value）之间的相似性Equation 3 来决定如何加权输入信息。具体来说，Attention的计算过程如下:\n",
    "\n",
    "\\boxed{\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V} \\tag{6}\\\n",
    "\n",
    "2.3.1 Self-Attention Layer\n",
    "2.3.2 Causal Self-Attention Layer\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}} \\textcolor{red}{+ M}\\right) V \\tag{7}\n",
    "\n",
    "2.3.3 Cross Attention Layer\n",
    "\\[ \\text{Attention}(Q_{dec}, \\textcolor{red}{K_{enc}}, \\textcolor{red}{V_{enc}}) = \\text{softmax}\\left(\\frac{Q_{dec} \\textcolor{red}{K_{enc}^\\top}}{\\sqrt{d_k}}\\right) \\textcolor{red}{V_{enc}} \\tag{8}\\]\n",
    "\n",
    "2.3.4 Time Complexity of Attention\n",
    "\\[ \\begin{array}{|l|l|} \\hline \\textbf{Step} & \\textbf{Time Complexity} \\\\ \\hline QK^\\top & \\mathcal{O}(n^2 d) \\\\ \\text{softmax}(QK^\\top) & \\mathcal{O}(n^2) \\\\ \\text{attention} \\times V & \\mathcal{O}(n^2 d) \\\\ \\hline \\textbf{Total} & \\mathcal{O}(n^2 d) \\\\ \\hline \\end{array} \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792c74b",
   "metadata": {},
   "source": [
    "#### 2.4 Normalization Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a02897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_hat + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93e1f5",
   "metadata": {},
   "source": [
    "#### 2.5 Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()   \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb9631",
   "metadata": {},
   "source": [
    "#### 2.6 Residual Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6a91d",
   "metadata": {},
   "source": [
    "#### 2.7 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ecbad",
   "metadata": {},
   "source": [
    "#### 2.8 Encoder & Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(config)\n",
    "        self.ffn = FeedForwardNetwork(config.d_model, config.d_ff)\n",
    "        self.norm1 = LayerNormalization(config.d_model)\n",
    "        self.norm2 = LayerNormalization(config.d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.self_attention(x, x, x))\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.causal_self_attention = CausalMultiHeadSelfAttention(config)\n",
    "        self.cross_attention = CrossAttention(config)\n",
    "        self.ffn = FeedForwardNetwork(config.d_model, config.d_ff)\n",
    "        self.norm1 = LayerNormalization(config.d_model)\n",
    "        self.norm2 = LayerNormalization(config.d_model)\n",
    "        self.norm3 = LayerNormalization(config.d_model)\n",
    "    def forward(self, y, x_enc):\n",
    "        y = self.norm1(y + self.causal_self_attention(y, y, y))\n",
    "        y = self.norm2(y + self.cross_attention(y, x_enc, x_enc))\n",
    "        y = self.norm3(y + self.ffn(y))\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_encoder_layers)])   \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_decoder_layers)])   \n",
    "    def forward(self, y, x_enc):\n",
    "        for layer in self.layers:\n",
    "            y = layer(y, x_enc)\n",
    "        return y\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n",
    "        ...\n",
    "    \n",
    "    def forward(self, original, target):\n",
    "        x_enc = self.encoder(original)\n",
    "        y_dec = self.decoder(target, x_enc)\n",
    "        output = self.output_layer(y_dec)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067d6f2",
   "metadata": {},
   "source": [
    "#### 2.11 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0.0, eps=1e-8):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]\n",
    "        self.t = 0  \n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        b1, b2 = self.betas\n",
    "\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            g = p.grad\n",
    "\n",
    "            # ----- weight decay -----\n",
    "            if self.weight_decay != 0.0:\n",
    "                    g = g.add(p.data, alpha=self.weight_decay)\n",
    "\n",
    "            # ----- Adam moments -----\n",
    "            self.m[i].mul_(b1).add_(g, alpha=(1.0 - b1))\n",
    "            self.v[i].mul_(b2).addcmul_(g, g, value=(1.0 - b2))\n",
    "\n",
    "            m_hat = self.m[i] / (1.0 - (b1 ** self.t))\n",
    "            v_hat = self.v[i] / (1.0 - (b2 ** self.t))\n",
    "\n",
    "            # update\n",
    "            p.data.addcdiv_(m_hat, v_hat.sqrt().add_(self.eps), value=-self.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                if set_to_none:\n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84790c",
   "metadata": {},
   "source": [
    "$$\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x}))$$\n",
    "\n",
    "$e^{i\\pi} + 1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403cd4d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
