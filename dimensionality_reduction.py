# -*- coding: utf-8 -*-
"""dimensionality_reduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_kdyXsA60djl-nTSUxLQTZuKcxkMA83

# Dimensionality reduction techniques comparison

### Dataset

For simplicity we use the Wine dataset from sklearn (UCI).

Features:
- alcohol
- malic_acid
- ash
- alcalinity_of_ash
- magnesium
- total_phenols
- flavanoids
- nonflavanoid_phenols
- proanthocyanins
- color_intensity
- hue
- od280od315_of_diluted_wines'
- proline

Labels: Wine classes
"""

from sklearn.datasets import load_wine
import numpy as np

data = load_wine()
features = data["data"]
labels = data["target"]
features.shape

import seaborn as sns
import matplotlib.pyplot as plt
try:
  import ipyvolume as ipv
except:
  !pip install ipyvolume -q
  import ipyvolume as ipv
  !jupyter nbextension enable --py --sys-prefix ipyvolume

def visualize_2d(x, labels):
    sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=labels, s=100, alpha=0.8,
                    palette="Greens", edgecolor="black")

def visualize_3d(x, labels):
    # Workaround as axis limits are not auto-scaling
    x_norm = (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))
    fig = ipv.figure(height=400, width=400)
    x, y, z = x_norm[:, 0], x_norm[:, 1], x_norm[:, 2]

    # Colors
    cmap = plt.get_cmap('Greens', 3)
    color = cmap(labels)
    ipv.scatter(x, y, z, size=4, marker="sphere", color=color)
    ipv.show()

"""### PCA"""

# It's also possible to manually compute eigenvectors and eigenvalues
subset_idx = 3
cov = np.cov(features)[:subset_idx, :subset_idx]
eig_vals, eig_vecs = np.linalg.eig(cov)
print("Eigenvalues: \n", eig_vals)
print("Eigenvectors: \n", eig_vecs)

"""2D Projection"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
# %time pca_2d = pca.fit_transform(features)
visualize_2d(x=pca_2d, labels=labels)

print("Eigenvalues: \n", pca.explained_variance_ratio_)
print("Eigenvectors: \n", pca.components_)

"""3D Projection"""

# Commented out IPython magic to ensure Python compatibility.
pca = PCA(n_components=3)
# %time pca_3d = pca.fit_transform(features)
visualize_3d(pca_3d, labels)

"""### MDS"""

# Precompute distance
from sklearn.metrics.pairwise import manhattan_distances

d_matrix = manhattan_distances(features)
d_matrix

import seaborn as sns
print("Distance matrix of the first 10 data points...")
distances = d_matrix[:10, :10]

mask = np.triu(np.ones_like(distances, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(distances, mask=mask, cmap=cmap, vmax=distances.max(), center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

# Commented out IPython magic to ensure Python compatibility.
from sklearn.manifold import MDS

mds = MDS(n_components=3,
          normalized_stress=False,
          metric=True,
          dissimilarity="precomputed",
          random_state=2023,
          eps=1e-9)
# %time mds_3d = mds.fit_transform(d_matrix)
visualize_3d(mds_3d, labels)

"""Note: It works better with Manhatten distances - most likely the high dimension is a problem using the euclidean distance"""

print("Raw stress: ", mds.stress_)

# Commented out IPython magic to ensure Python compatibility.
mds = MDS(n_components=2, normalized_stress=False, eps=1e-9)
# %time mds_2d = mds.fit_transform(features)
visualize_2d(mds_2d, labels)

"""### TSNE"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.manifold import TSNE

# %time tsne_3d = TSNE(n_components=3, perplexity=10, early_exaggeration=12, learning_rate='auto', init='pca', n_jobs=4).fit_transform(features)
visualize_3d(tsne_3d, labels)

# Commented out IPython magic to ensure Python compatibility.
# %time tsne_2d = TSNE(n_components=2, perplexity=10, early_exaggeration=12, learning_rate='auto', init='random', n_jobs=4).fit_transform(features)
visualize_2d(tsne_2d, labels)

"""### UMAP"""

# Commented out IPython magic to ensure Python compatibility.
try:
  import umap.umap_ as umap
except:
  !pip install umap-learn -q
  import umap.umap_ as umap


reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric='euclidean')
# %time umap_3d = reducer.fit_transform(features)
visualize_3d(umap_3d, labels)

# Commented out IPython magic to ensure Python compatibility.
reducer = reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean')
# %time umap_2d = reducer.fit_transform(features)
visualize_2d(umap_2d, labels)